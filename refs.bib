
@inproceedings{cudeiroCaptureLearningSynthesis2019,
  title = {Capture, {{Learning}}, and {{Synthesis}} of {{3D Speaking Styles}}},
  shorttitle = {{{VOCA}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cudeiro, Daniel and Bolkart, Timo and Laidlaw, Cassidy and Ranjan, Anurag and Black, Michael J.},
  year = {2019},
  pages = {10101--10111},
  publisher = {{IEEE}},
  file = {C\:\\Users\\woong\\Zotero\\storage\\BQXH8S3D\\Cudeiro 등 - 2019 - Capture, Learning, and Synthesis of 3D Speaking St.pdf}
}

@article{edwardsJALIAnimatorcentricViseme2016,
  title = {{{JALI}}: An Animator-Centric Viseme Model for Expressive Lip Synchronization},
  shorttitle = {{{JALI}}},
  author = {Edwards, Pif and Landreth, Chris and Fiume, Eugene and Singh, Karan},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2897824.2925984},
  abstract = {The rich signals we extract from facial expressions imposes high expectations for the science and art of facial animation. While the advent of high-resolution performance capture has greatly improved realism, the utility of procedural animation warrants a prominent place in facial animation workflow. We present a system that, given an input audio soundtrack and speech transcript, automatically generates expressive lip-synchronized facial animation that is amenable to further artistic refinement, and that is comparable with both performance capture and professional animator output. Because of the diversity of ways we produce sound, the mapping from phonemes to visual depictions as visemes is manyvalued. We draw from psycholinguistics to capture this variation using two visually distinct anatomical actions: Jaw and Lip, where sound is primarily controlled by jaw articulation and lower-face muscles, respectively. We describe the construction of a transferable template JALI 3D facial rig, built upon the popular facial muscle action unit representation FACS. We show that acoustic properties in a speech signal map naturally to the dynamic degree of jaw and lip in visual speech. We provide an array of compelling animation clips, compare against performance capture and existing procedural animation, and report on a brief user study.},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\JA5ELCHE\\Edwards 등 - 2016 - JALI an animator-centric viseme model for express.pdf}
}

@inproceedings{ezzatMikeTalkTalkingFacial1998,
  title = {{{MikeTalk}}: A Talking Facial Display Based on Morphing Visemes},
  shorttitle = {{{MikeTalk}}},
  booktitle = {Proceedings {{Computer Animation}} '98 ({{Cat}}. {{No}}.{{98EX169}})},
  author = {Ezzat, T. and Poggio, T.},
  year = {1998},
  month = jun,
  pages = {96--102},
  issn = {1087-4844},
  doi = {10.1109/CA.1998.681913},
  abstract = {We present MikeTalk, a text-to-audiovisual speech synthesizer which converts input text into an audiovisual speech stream. MikeTalk is built using visemes, which are a set of images spanning a large range of mouth shapes. The visemes are acquired from a recorded visual corpus of a human subject which is specifically designed to elicit one instantiation of each viseme. Using optical flow methods, correspondence from every viseme to every other viseme is computed automatically. By morphing along this correspondence, a smooth transition between viseme images may be generated. A complete visual utterance is constructed by concatenating viseme transitions. Finally, phoneme and timing information extracted from a text-to-speech synthesizer is exploited to determine which viseme transitions to use, and the rate at which the morphing process should occur. In this manner, we are able to synchronize the visual speech stream with the audio speech stream, and hence give the impression, of a photorealistic talking face.},
  keywords = {Auditory displays,Humans,Image converters,Image motion analysis,Mouth,Optical recording,Shape,Speech synthesis,Streaming media,Synthesizers},
  file = {C\:\\Users\\woong\\Zotero\\storage\\BFKUR64B\\Ezzat and Poggio - 1998 - MikeTalk a talking facial display based on morphi.pdf;C\:\\Users\\woong\\Zotero\\storage\\DBJLEAQM\\681913.html}
}

@article{hansonUpendingUncannyValley,
  title = {Upending the {{Uncanny Valley}}},
  author = {Hanson, David},
  pages = {8},
  abstract = {Although robotics researchers commonly contend that robots should not look too humanlike, many artforms have successfully depicted people and have come to be accepted as great and important works, with examples such as Rodin's Thinker, Mary Cassat's infants, and Disney's Abe Lincoln simulacrum. Extending this tradition to intelligent robotics, the authors have depicted late sci-fi writer Philip K Dick with an autonomous, intelligent android. In doing so, the authors aspire to bring robotic systems up to the level of great art, while using the technology as a mirror for examining human nature in social AI development and cognitive science experiments..},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\387WHEKD\\Hanson - Upending the Uncanny Valley.pdf}
}

@article{karrasAudiodrivenFacialAnimation2017,
  title = {Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion},
  shorttitle = {{{NVIDIA}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Herva, Antti and Lehtinen, Jaakko},
  year = {2017},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {4},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3072959.3073658},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\U9L63V2Y\\Karras 등 - 2017 - Audio-driven facial animation by joint end-to-end .pdf}
}

@article{potamiasLearningGenerateCustomized2020,
  title = {Learning to {{Generate Customized Dynamic 3D Facial Expressions}}},
  shorttitle = {{{CD3DExpression}}},
  author = {Potamias, Rolandos Alexandros and Zheng, Jiali and Ploumpis, Stylianos and Bouritsas, Giorgos and Ververas, Evangelos and Zafeiriou, Stefanos},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.09805 [cs]},
  eprint = {2007.09805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent advances in deep learning have significantly pushed the state-of-the-art in photorealistic video animation given a single image. In this paper, we extrapolate those advances to the 3D domain, by studying 3D image-to-video translation with a particular focus on 4D facial expressions. Although 3D facial generative models have been widely explored during the past years, 4D animation remains relatively unexplored. To this end, in this study we employ a deep mesh encoder-decoder like architecture to synthesize realistic high resolution facial expressions by using a single neutral frame along with an expression identification. In addition, processing 3D meshes remains a non-trivial task compared to data that live on grid-like structures, such as images. Given the recent progress in mesh processing with graph convolutions, we make use of a recently introduced learnable operator which acts directly on the mesh structure by taking advantage of local vertex orderings. In order to generalize to 4D facial expressions across subjects, we trained our model using a high resolution dataset with 4D scans of six facial expressions from 180 subjects. Experimental results demonstrate that our approach preserves the subject's identity information even for unseen subjects and generates high quality expressions. To the best of our knowledge, this is the first study tackling the problem of 4D facial expression synthesis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\woong\\Zotero\\storage\\X3Z3XZUF\\Potamias 등 - 2020 - Learning to Generate Customized Dynamic 3D Facial .pdf}
}

@inproceedings{richardMeshTalk3DFace2021,
  title = {{{MeshTalk}}: {{3D Face Animation From Speech Using Cross-Modality Disentanglement}}},
  shorttitle = {{{MeshTalk}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Richard, Alexander and Zollh{\"o}fer, Michael and Wen, Yandong and {de la Torre}, Fernando and Sheikh, Yaser},
  year = {2021},
  pages = {1173--1182},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\PLTIJDPZ\\Richard 등 - 2021 - MeshTalk 3D Face Animation From Speech Using Cros.pdf;C\:\\Users\\woong\\Zotero\\storage\\3G2TWNAW\\Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_p.html}
}

@article{zhouVisemenetAudiodrivenAnimatorcentric2018,
  title = {Visemenet: Audio-Driven Animator-Centric Speech Animation},
  shorttitle = {Visemenet},
  author = {Zhou, Yang and Xu, Zhan and Landreth, Chris and Kalogerakis, Evangelos and Maji, Subhransu and Singh, Karan},
  year = {2018},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {37},
  number = {4},
  pages = {161:1--161:10},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201292},
  abstract = {We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.},
  keywords = {facial animation,neural networks},
  file = {C\:\\Users\\woong\\Zotero\\storage\\G9U5FXT4\\Zhou et al. - 2018 - Visemenet audio-driven animator-centric speech an.pdf}
}


