
@misc{AccuracyPrecisionPosition,
  title = {The {{Accuracy}} and {{Precision}} of {{Position}} and {{Orientation Tracking}} in the {{HTC Vive Virtual Reality System}} for {{Scientific Research}} - {{Diederick C}}. {{Niehorster}}, {{Li Li}}, {{Markus Lappe}}, 2017},
  howpublished = {https://journals.sagepub.com/doi/full/10.1177/2041669517708205}
}

@article{casermanRealtimeBodyTracking2019,
  title = {Real-Time Body Tracking in Virtual Reality Using a {{Vive}} Tracker},
  author = {Caserman, Polona and {Garcia-Agundez}, Augusto and Konrad, Robert and G{\"o}bel, Stefan and Steinmetz, Ralf},
  year = {2019},
  month = jun,
  journal = {Virtual Reality},
  volume = {23},
  number = {2},
  pages = {155--168},
  issn = {1359-4338, 1434-9957},
  doi = {10.1007/s10055-018-0374-z},
  abstract = {Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these applications is to enhance the sense of belief that the user is ``present'' in the virtual environment. By tracking the user's skeleton in real-time, it is possible to synchronize the avatar's motions with the user's motions. Although current common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to the lack of positional and rotation data, the current VR applications typically do not represent the user's motions. In this paper, we present an accurate, low-latency body tracking approach for VR-based applications using Vive Trackers. Using a HTC Vive headset and Vive Trackers, we have been able to create an immersive VR experience, by animating the motions of the avatar as smoothly, rapidly and as accurately as possible. An evaluation showed our solution is capable of tracking both joint rotation and position with reasonable accuracy and a very low end-to-latency of 6.71 {$\pm$} 0.80 ms{$\mkern1mu$}. Due to this merely imperceptible delay and precise tracking, our solution can show the movements of the user in real-time in order to create deeper immersion.},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\MDB89WEY\\Caserman et al. - 2019 - Real-time body tracking in virtual reality using a.pdf}
}

@article{casermanRealtimeBodyTracking2019a,
  title = {Real-Time Body Tracking in Virtual Reality Using a {{Vive}} Tracker},
  author = {Caserman, Polona and {Garcia-Agundez}, Augusto and Konrad, Robert and G{\"o}bel, Stefan and Steinmetz, Ralf},
  year = {2019},
  month = jun,
  journal = {Virtual Reality},
  volume = {23},
  number = {2},
  pages = {155--168},
  issn = {1434-9957},
  doi = {10.1007/s10055-018-0374-z},
  abstract = {Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these applications is to enhance the sense of belief that the user is ``present'' in the virtual environment. By tracking the user's skeleton in real-time, it is possible to synchronize the avatar's motions with the user's motions. Although current common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to the lack of positional and rotation data, the current VR applications typically do not represent the user's motions. In this paper, we present an accurate, low-latency body tracking approach for VR-based applications using Vive Trackers. Using a HTC Vive headset and Vive Trackers, we have been able to create an immersive VR experience, by animating the motions of the avatar as smoothly, rapidly and as accurately as possible. An evaluation showed our solution is capable of tracking both joint rotation and position with reasonable accuracy and a very low end-to-latency of \$\$6.71 \textbackslash pm 0.80\textbackslash hbox \{ ms\}\$\$. Due to this merely imperceptible delay and precise tracking, our solution can show the movements of the user in real-time~in order to create deeper immersion.},
  langid = {english},
  keywords = {Full-body avatar,HTC Vive tracker,Inverse kinematics,Low-latency,Real-time tracking,Virtual reality},
  file = {C\:\\Users\\woong\\Zotero\\storage\\6Z6HU64V\\Caserman et al. - 2019 - Real-time body tracking in virtual reality using a.pdf}
}

@inproceedings{cheng4DFABLargeScale2018,
  title = {{{4DFAB}}: {{A Large Scale 4D Database}} for {{Facial Expression Analysis}} and {{Biometric Applications}}},
  shorttitle = {{{4DFAB}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou, Stefanos},
  year = {2018},
  pages = {5117--5126},
  file = {C\:\\Users\\woong\\Zotero\\storage\\RK4UFZFC\\Cheng 등 - 2018 - 4DFAB A Large Scale 4D Database for Facial Expres.pdf;C\:\\Users\\woong\\Zotero\\storage\\HFLY523Y\\Cheng_4DFAB_A_Large_CVPR_2018_paper.html}
}

@article{cheng4DFABLargeScale2018a,
  title = {{{4DFAB}}: {{A Large Scale 4D Facial Expression Database}} for {{Biometric Applications}}},
  shorttitle = {{{4DFAB}}\_{{DT}}},
  author = {Cheng, Shiyang and Kotsia, Irene and Pantic, Maja and Zafeiriou, Stefanos},
  year = {2018},
  month = jun,
  journal = {arXiv:1712.01443 [cs]},
  eprint = {1712.01443},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\woong\\Zotero\\storage\\55DKMPT3\\Cheng et al. - 2018 - 4DFAB A Large Scale 4D Facial Expression Database.pdf;C\:\\Users\\woong\\Zotero\\storage\\L9YMD22F\\1712.html}
}

@inproceedings{cudeiroCaptureLearningSynthesis2019,
  title = {Capture, {{Learning}}, and {{Synthesis}} of {{3D Speaking Styles}}},
  shorttitle = {{{VOCA}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cudeiro, Daniel and Bolkart, Timo and Laidlaw, Cassidy and Ranjan, Anurag and Black, Michael J.},
  year = {2019},
  pages = {10101--10111},
  publisher = {{IEEE}},
  file = {C\:\\Users\\woong\\Zotero\\storage\\BQXH8S3D\\Cudeiro 등 - 2019 - Capture, Learning, and Synthesis of 3D Speaking St.pdf}
}

@article{edwardsJALIAnimatorcentricViseme2016,
  title = {{{JALI}}: An Animator-Centric Viseme Model for Expressive Lip Synchronization},
  shorttitle = {{{JALI}}},
  author = {Edwards, Pif and Landreth, Chris and Fiume, Eugene and Singh, Karan},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2897824.2925984},
  abstract = {The rich signals we extract from facial expressions imposes high expectations for the science and art of facial animation. While the advent of high-resolution performance capture has greatly improved realism, the utility of procedural animation warrants a prominent place in facial animation workflow. We present a system that, given an input audio soundtrack and speech transcript, automatically generates expressive lip-synchronized facial animation that is amenable to further artistic refinement, and that is comparable with both performance capture and professional animator output. Because of the diversity of ways we produce sound, the mapping from phonemes to visual depictions as visemes is manyvalued. We draw from psycholinguistics to capture this variation using two visually distinct anatomical actions: Jaw and Lip, where sound is primarily controlled by jaw articulation and lower-face muscles, respectively. We describe the construction of a transferable template JALI 3D facial rig, built upon the popular facial muscle action unit representation FACS. We show that acoustic properties in a speech signal map naturally to the dynamic degree of jaw and lip in visual speech. We provide an array of compelling animation clips, compare against performance capture and existing procedural animation, and report on a brief user study.},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\JA5ELCHE\\Edwards 등 - 2016 - JALI an animator-centric viseme model for express.pdf}
}

@inproceedings{eubanksEffectsBodyTracking2020,
  title = {The {{Effects}} of {{Body Tracking Fidelity}} on {{Embodiment}} of an {{Inverse-Kinematic Avatar}} for {{Male Participants}}},
  booktitle = {2020 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Eubanks, James Coleman and Moore, Alec G. and Fishwick, Paul A. and McMahan, Ryan P.},
  year = {2020},
  month = nov,
  pages = {54--63},
  issn = {1554-7868},
  doi = {10.1109/ISMAR50242.2020.00025},
  abstract = {Many research studies have investigated avatar embodiment and its effects on self-location, agency, and body ownership. Researchers have also investigated the effects of various external stimuli and avatar appearances during embodiment. However, the effects of body tracking fidelity while embodying an inverse-kinematic avatar are relatively unexplored. In this paper, we present two studies using a set of six trackers that investigate four levels of body tracking fidelity during avatar embodiment for male participants only: Complete (head, hands, feet, and pelvis trackers), Head-and-Extremities (head, hands, and feet trackers), Head-and-Hands (head and hands trackers), and No-Avatar (head and hands trackers; only controllers visible). Our results indicate that tracking the head, hands, and feet significantly increases the sense of embodiment and the sense of spatial presence when embodying an inverse-kinematic avatar for male participants.},
  keywords = {Augmented reality,Avatars,avatars. Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality,body tracking fidelity,Design methodology,Embodiment,Embodiment; virtual reality; body tracking fidelity; avatars. Human-centered computing,External stimuli,HCI design and evaluation methods,Human computer interaction (HCI),Human-centered computing,Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies,Interaction paradigms,Pelvis,User studies,virtual reality,Virtual reality},
  file = {C\:\\Users\\woong\\Zotero\\storage\\UDPK7QD2\\Eubanks et al. - 2020 - The Effects of Body Tracking Fidelity on Embodimen.pdf;C\:\\Users\\woong\\Zotero\\storage\\48QV7747\\9284776.html}
}

@inproceedings{eubanksEffectsBodyTracking2020a,
  title = {The {{Effects}} of {{Body Tracking Fidelity}} on {{Embodiment}} of an {{Inverse-Kinematic Avatar}} for {{Male Participants}}},
  booktitle = {2020 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Eubanks, James Coleman and Moore, Alec G. and Fishwick, Paul A. and McMahan, Ryan P.},
  year = {2020},
  month = nov,
  pages = {54--63},
  issn = {1554-7868},
  doi = {10.1109/ISMAR50242.2020.00025},
  abstract = {Many research studies have investigated avatar embodiment and its effects on self-location, agency, and body ownership. Researchers have also investigated the effects of various external stimuli and avatar appearances during embodiment. However, the effects of body tracking fidelity while embodying an inverse-kinematic avatar are relatively unexplored. In this paper, we present two studies using a set of six trackers that investigate four levels of body tracking fidelity during avatar embodiment for male participants only: Complete (head, hands, feet, and pelvis trackers), Head-and-Extremities (head, hands, and feet trackers), Head-and-Hands (head and hands trackers), and No-Avatar (head and hands trackers; only controllers visible). Our results indicate that tracking the head, hands, and feet significantly increases the sense of embodiment and the sense of spatial presence when embodying an inverse-kinematic avatar for male participants.},
  keywords = {Augmented reality,Avatars,avatars. Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality,body tracking fidelity,Design methodology,Embodiment,Embodiment; virtual reality; body tracking fidelity; avatars. Human-centered computing,External stimuli,HCI design and evaluation methods,Human computer interaction (HCI),Human-centered computing,Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies,Interaction paradigms,Pelvis,User studies,virtual reality,Virtual reality},
  file = {C\:\\Users\\woong\\Zotero\\storage\\H395CCKT\\Eubanks et al. - 2020 - The Effects of Body Tracking Fidelity on Embodimen.pdf;C\:\\Users\\woong\\Zotero\\storage\\KWTAZFX9\\9284776.html}
}

@inproceedings{eubanksEffectsBodyTracking2020b,
  title = {The {{Effects}} of {{Body Tracking Fidelity}} on {{Embodiment}} of an {{Inverse-Kinematic Avatar}} for {{Male Participants}}},
  booktitle = {2020 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Eubanks, James Coleman and Moore, Alec G. and Fishwick, Paul A. and McMahan, Ryan P.},
  year = {2020},
  month = nov,
  pages = {54--63},
  issn = {1554-7868},
  doi = {10.1109/ISMAR50242.2020.00025},
  abstract = {Many research studies have investigated avatar embodiment and its effects on self-location, agency, and body ownership. Researchers have also investigated the effects of various external stimuli and avatar appearances during embodiment. However, the effects of body tracking fidelity while embodying an inverse-kinematic avatar are relatively unexplored. In this paper, we present two studies using a set of six trackers that investigate four levels of body tracking fidelity during avatar embodiment for male participants only: Complete (head, hands, feet, and pelvis trackers), Head-and-Extremities (head, hands, and feet trackers), Head-and-Hands (head and hands trackers), and No-Avatar (head and hands trackers; only controllers visible). Our results indicate that tracking the head, hands, and feet significantly increases the sense of embodiment and the sense of spatial presence when embodying an inverse-kinematic avatar for male participants.},
  keywords = {Augmented reality,Avatars,avatars. Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality,body tracking fidelity,Design methodology,Embodiment,Embodiment; virtual reality; body tracking fidelity; avatars. Human-centered computing,External stimuli,HCI design and evaluation methods,Human computer interaction (HCI),Human-centered computing,Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies,Interaction paradigms,Pelvis,User studies,virtual reality,Virtual reality},
  file = {C\:\\Users\\woong\\Zotero\\storage\\PCQWULKS\\Eubanks et al. - 2020 - The Effects of Body Tracking Fidelity on Embodimen.pdf;C\:\\Users\\woong\\Zotero\\storage\\3KGL3YX2\\9284776.html}
}

@inproceedings{ezzatMikeTalkTalkingFacial1998,
  title = {{{MikeTalk}}: A Talking Facial Display Based on Morphing Visemes},
  shorttitle = {{{MikeTalk}}},
  booktitle = {Proceedings {{Computer Animation}} '98 ({{Cat}}. {{No}}.{{98EX169}})},
  author = {Ezzat, T. and Poggio, T.},
  year = {1998},
  month = jun,
  pages = {96--102},
  issn = {1087-4844},
  doi = {10.1109/CA.1998.681913},
  abstract = {We present MikeTalk, a text-to-audiovisual speech synthesizer which converts input text into an audiovisual speech stream. MikeTalk is built using visemes, which are a set of images spanning a large range of mouth shapes. The visemes are acquired from a recorded visual corpus of a human subject which is specifically designed to elicit one instantiation of each viseme. Using optical flow methods, correspondence from every viseme to every other viseme is computed automatically. By morphing along this correspondence, a smooth transition between viseme images may be generated. A complete visual utterance is constructed by concatenating viseme transitions. Finally, phoneme and timing information extracted from a text-to-speech synthesizer is exploited to determine which viseme transitions to use, and the rate at which the morphing process should occur. In this manner, we are able to synchronize the visual speech stream with the audio speech stream, and hence give the impression, of a photorealistic talking face.},
  keywords = {Auditory displays,Humans,Image converters,Image motion analysis,Mouth,Optical recording,Shape,Speech synthesis,Streaming media,Synthesizers},
  file = {C\:\\Users\\woong\\Zotero\\storage\\BFKUR64B\\Ezzat and Poggio - 1998 - MikeTalk a talking facial display based on morphi.pdf;C\:\\Users\\woong\\Zotero\\storage\\DBJLEAQM\\681913.html}
}

@inproceedings{guoEffectsAvatarsPresence2014,
  title = {The Effects of Avatars on Presence in Virtual Environments for Persons with Mobility Impairments},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Artificial Reality}} and {{Telexistence}} and the 19th {{Eurographics Symposium}} on {{Virtual Environments}}},
  author = {Guo, R. and Samaraweera, G. and Quarles, J.},
  year = {12월 8, 2014},
  series = {{{ICAT}} - {{EGVE}} '14},
  pages = {1--8},
  publisher = {{Eurographics Association}},
  address = {{Goslar, DEU}},
  abstract = {The main question we ask is: How do avatars affect presence specifically for Persons with Mobility Impairments (PMIs)? For example, PMIs' deficits in the proprioceptive sense could affect their body perception in immersive virtual reality, which could impact presence. To investigate this we replicated the classic virtual pit experiment and included a responsive full body avatar (or lack thereof) as a 3D user interface. We recruited from two different populations: 11 PMIs and another 11 Persons without Mobility Impairments (PNMIs) as a control. Each PNMI was matched to a PMI based on age, weight, height, and prior VE exposure. Results of this study indicate that avatars elicit a higher sense of presence for PMIs than for PNMIs. In addition, results suggest that PMIs are easier to immerse in VEs than PNMIs, which may further motivate the future use of VE technology for PMIs.},
  isbn = {978-3-905674-65-1}
}

@article{hansonUpendingUncannyValley,
  title = {Upending the {{Uncanny Valley}}},
  author = {Hanson, David},
  pages = {8},
  abstract = {Although robotics researchers commonly contend that robots should not look too humanlike, many artforms have successfully depicted people and have come to be accepted as great and important works, with examples such as Rodin's Thinker, Mary Cassat's infants, and Disney's Abe Lincoln simulacrum. Extending this tradition to intelligent robotics, the authors have depicted late sci-fi writer Philip K Dick with an autonomous, intelligent android. In doing so, the authors aspire to bring robotic systems up to the level of great art, while using the technology as a mirror for examining human nature in social AI development and cognitive science experiments..},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\387WHEKD\\Hanson - Upending the Uncanny Valley.pdf}
}

@article{hansonUpendingUncannyValleya,
  title = {Upending the {{Uncanny Valley}}},
  author = {Hanson, David},
  pages = {8},
  abstract = {Although robotics researchers commonly contend that robots should not look too humanlike, many artforms have successfully depicted people and have come to be accepted as great and important works, with examples such as Rodin's Thinker, Mary Cassat's infants, and Disney's Abe Lincoln simulacrum. Extending this tradition to intelligent robotics, the authors have depicted late sci-fi writer Philip K Dick with an autonomous, intelligent android. In doing so, the authors aspire to bring robotic systems up to the level of great art, while using the technology as a mirror for examining human nature in social AI development and cognitive science experiments..},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\5V5ZYKAK\\Hanson - Upending the Uncanny Valley.pdf}
}

@article{karrasAudiodrivenFacialAnimation2017,
  title = {Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion},
  shorttitle = {{{NVIDIA}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Herva, Antti and Lehtinen, Jaakko},
  year = {2017},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {4},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3072959.3073658},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\U9L63V2Y\\Karras 등 - 2017 - Audio-driven facial animation by joint end-to-end .pdf}
}

@article{liLearningModelFacial2017,
  title = {Learning a Model of Facial Shape and Expression from {{4D}} Scans},
  shorttitle = {{{FLAME}}},
  author = {Li, Tianye and Bolkart, Timo and Black, Michael J. and Li, Hao and Romero, Javier},
  year = {2017},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {6},
  pages = {1--17},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3130800.3130813},
  abstract = {The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The pose and expression dependent articulations are learned from 4D face sequences in the D3DFACS dataset along with additional 4D sequences. We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\F4IAXX6A\\Li 등 - 2017 - Learning a model of facial shape and expression fr.pdf}
}

@article{liuBEATLargeScaleSemantic,
  title = {{{BEAT}}: {{A Large-Scale Semantic}} and {{Emotional Multi-Modal Dataset}} for {{Conversational Gestures Synthesis}}},
  shorttitle = {{{BEAT}}},
  author = {Liu, Haiyang and Zhu, Zihao and Iwamoto, Naoya and Peng, Yichen and Li, Zhengqing and Zhou, You and Bozkurt, Elif and Zheng, Bo},
  pages = {27},
  abstract = {Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem, due to the lack of available datasets, models and standard evaluation metrics. To address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics, in addition to the known correlation with audio, text, and speaker identity. Based on this observation, we propose a baseline model, Cascaded Motion Network (CaMN), which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the diversity of synthesized gestures, we introduce a metric, Semantic Relevance Gesture Recall (SRGR). Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance. To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields including controllable gesture synthesis, cross-modality analysis, emotional gesture recognition. The data, code and model will be released for research.},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\V4S6BSI8\\Liu 등 - BEAT A Large-Scale Semantic and Emotional Multi-M.pdf}
}

@article{lombardiDeepAppearanceModels2018,
  title = {Deep Appearance Models for Face Rendering},
  shorttitle = {{{DN Avatar}}},
  author = {Lombardi, Stephen and Saragih, Jason and Simon, Tomas and Sheikh, Yaser},
  year = {7월 30, 2018},
  journal = {ACM Transactions on Graphics},
  volume = {37},
  number = {4},
  pages = {68:1--68:13},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201401},
  abstract = {We introduce a deep appearance model for rendering the human face. Inspired by Active Appearance Models, we develop a data-driven rendering pipeline that learns a joint representation of facial geometry and appearance from a multiview capture setup. Vertex positions and view-specific textures are modeled using a deep variational autoencoder that captures complex nonlinear effects while producing a smooth and compact latent representation. View-specific texture enables the modeling of view-dependent effects such as specularity. In addition, it can also correct for imperfect geometry stemming from biased or low resolution estimates. This is a significant departure from the traditional graphics pipeline, which requires highly accurate geometry as well as all elements of the shading model to achieve realism through physically-inspired light transport. Acquiring such a high level of accuracy is difficult in practice, especially for complex and intricate parts of the face, such as eyelashes and the oral cavity. These are handled naturally by our approach, which does not rely on precise estimates of geometry. Instead, the shading model accommodates deficiencies in geometry though the flexibility afforded by the neural network employed. At inference time, we condition the decoding network on the viewpoint of the camera in order to generate the appropriate texture for rendering. The resulting system can be implemented simply using existing rendering engines through dynamic textures with flat lighting. This representation, together with a novel unsupervised technique for mapping images to facial states, results in a system that is naturally suited to real-time interactive settings such as Virtual Reality (VR).},
  keywords = {appearance models,deep appearance models,face rendering,image-based rendering},
  file = {C\:\\Users\\woong\\Zotero\\storage\\KNUJC9MD\\Lombardi 등 - 2018 - Deep appearance models for face rendering.pdf}
}

@article{niehorsterAccuracyPrecisionPosition2017,
  title = {The {{Accuracy}} and {{Precision}} of {{Position}} and {{Orientation Tracking}} in the {{HTC Vive Virtual Reality System}} for {{Scientific Research}}},
  author = {Niehorster, Diederick C. and Li, Li and Lappe, Markus},
  year = {2017},
  month = jun,
  journal = {i-Perception},
  volume = {8},
  number = {3},
  pages = {2041669517708205},
  publisher = {{SAGE Publications}},
  issn = {2041-6695},
  doi = {10.1177/2041669517708205},
  abstract = {The advent of inexpensive consumer virtual reality equipment enables many more researchers to study perception with naturally moving observers. One such system, the HTC Vive, offers a large field-of-view, high-resolution head mounted display together with a room-scale tracking system for less than a thousand U.S. dollars. If the position and orientation tracking of this system is of sufficient accuracy and precision, it could be suitable for much research that is currently done with far more expensive systems. Here we present a quantitative test of the HTC Vive?s position and orientation tracking as well as its end-to-end system latency. We report that while the precision of the Vive?s tracking measurements is high and its system latency (22?ms) is low, its position and orientation measurements are provided in a coordinate system that is tilted with respect to the physical ground plane. Because large changes in offset were found whenever tracking was briefly lost, it cannot be corrected for with a one-time calibration procedure. We conclude that the varying offset between the virtual and the physical tracking space makes the HTC Vive at present unsuitable for scientific experiments that require accurate visual stimulation of self-motion through a virtual world. It may however be suited for other experiments that do not have this requirement.},
  file = {C\:\\Users\\woong\\Zotero\\storage\\L37U4IKX\\Niehorster 등 - 2017 - The Accuracy and Precision of Position and Orienta.pdf}
}

@article{potamiasLearningGenerateCustomized2020,
  title = {Learning to {{Generate Customized Dynamic 3D Facial Expressions}}},
  shorttitle = {{{CD3DExpression}}},
  author = {Potamias, Rolandos Alexandros and Zheng, Jiali and Ploumpis, Stylianos and Bouritsas, Giorgos and Ververas, Evangelos and Zafeiriou, Stefanos},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.09805 [cs]},
  eprint = {2007.09805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent advances in deep learning have significantly pushed the state-of-the-art in photorealistic video animation given a single image. In this paper, we extrapolate those advances to the 3D domain, by studying 3D image-to-video translation with a particular focus on 4D facial expressions. Although 3D facial generative models have been widely explored during the past years, 4D animation remains relatively unexplored. To this end, in this study we employ a deep mesh encoder-decoder like architecture to synthesize realistic high resolution facial expressions by using a single neutral frame along with an expression identification. In addition, processing 3D meshes remains a non-trivial task compared to data that live on grid-like structures, such as images. Given the recent progress in mesh processing with graph convolutions, we make use of a recently introduced learnable operator which acts directly on the mesh structure by taking advantage of local vertex orderings. In order to generalize to 4D facial expressions across subjects, we trained our model using a high resolution dataset with 4D scans of six facial expressions from 180 subjects. Experimental results demonstrate that our approach preserves the subject's identity information even for unseen subjects and generates high quality expressions. To the best of our knowledge, this is the first study tackling the problem of 4D facial expression synthesis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\woong\\Zotero\\storage\\X3Z3XZUF\\Potamias 등 - 2020 - Learning to Generate Customized Dynamic 3D Facial .pdf}
}

@inproceedings{richardAudioGazedrivenFacial2021,
  title = {Audio- and {{Gaze-driven Facial Animation}} of {{Codec Avatars}}},
  shorttitle = {{{AG Driven}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Richard, Alexander and Lea, Colin and Ma, Shugao and Gall, Juergen and de {la Torre}, Fernando and Sheikh, Yaser},
  year = {2021},
  month = jan,
  pages = {41--50},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00009},
  isbn = {978-1-66540-477-8},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\AXPFB974\\Richard 등 - 2021 - Audio- and Gaze-driven Facial Animation of Codec A.pdf}
}

@inproceedings{richardMeshTalk3DFace2021,
  title = {{{MeshTalk}}: {{3D Face Animation From Speech Using Cross-Modality Disentanglement}}},
  shorttitle = {{{MeshTalk}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Richard, Alexander and Zollh{\"o}fer, Michael and Wen, Yandong and {de la Torre}, Fernando and Sheikh, Yaser},
  year = {2021},
  pages = {1173--1182},
  langid = {english},
  file = {C\:\\Users\\woong\\Zotero\\storage\\PLTIJDPZ\\Richard 등 - 2021 - MeshTalk 3D Face Animation From Speech Using Cros.pdf;C\:\\Users\\woong\\Zotero\\storage\\3G2TWNAW\\Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_p.html}
}

@article{songAccurateFaceRig2020,
  title = {Accurate {{Face Rig Approximation}} with {{Deep Differential Subspace Reconstruction}}},
  shorttitle = {Tlqkf},
  author = {Song, Steven L. and Shi, Weiqi and Reed, Michael},
  year = {2020},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {39},
  number = {4},
  eprint = {2006.01746},
  eprinttype = {arxiv},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3386569.3392491},
  abstract = {To be suitable for film-quality animation, rigs for character deformation must fulfill a broad set of requirements. They must be able to create highly stylized deformation, allow a wide variety of controls to permit artistic freedom, and accurately reflect the design intent. Facial deformation is especially challenging due to its nonlinearity with respect to the animation controls and its additional precision requirements, which often leads to highly complex face rigs that are not generalizable to other characters. This lack of generality creates a need for approximation methods that encode the deformation in simpler structures. We propose a rig approximation method that addresses these issues by learning localized shape information in differential coordinates and, separately, a subspace for mesh reconstruction. The use of differential coordinates produces a smooth distribution of errors in the resulting deformed surface, while the learned subspace provides constraints that reduce the low frequency error in the reconstruction. Our method can reconstruct both face and body deformations with high fidelity and does not require a set of well-posed animation examples, as we demonstrate with a variety of production characters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning},
  file = {C\:\\Users\\woong\\Zotero\\storage\\DGRFAECB\\Song et al. - 2020 - Accurate Face Rig Approximation with Deep Differen.pdf;C\:\\Users\\woong\\Zotero\\storage\\435DUC62\\2006.html}
}

@article{taylorDeepLearningApproach2017,
  title = {A Deep Learning Approach for Generalized Speech Animation},
  author = {Taylor, Sarah and Kim, Taehwan and Yue, Yisong and Mahler, Moshe and Krahe, James and Rodriguez, Anastasio Garcia and Hodgins, Jessica and Matthews, Iain},
  year = {7월 20, 2017},
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {4},
  pages = {93:1--93:11},
  issn = {0730-0301},
  doi = {10.1145/3072959.3073699},
  abstract = {We introduce a simple and effective deep learning approach to automatically generate natural looking speech animation that synchronizes to input speech. Our approach uses a sliding window predictor that learns arbitrary nonlinear mappings from phoneme label input sequences to mouth movements in a way that accurately captures natural motion and visual coarticulation effects. Our deep learning approach enjoys several attractive properties: it runs in real-time, requires minimal parameter tuning, generalizes well to novel input speech sequences, is easily edited to create stylized and emotional speech, and is compatible with existing animation retargeting approaches. One important focus of our work is to develop an effective approach for speech animation that can be easily integrated into existing production pipelines. We provide a detailed description of our end-to-end approach, including machine learning design decisions. Generalized speech animation results are demonstrated over a wide range of animation clips on a variety of characters and voices, including singing and foreign language input. Our approach can also generate on-demand speech animation in real-time from user speech input.},
  keywords = {machine learning,speech animation},
  file = {C\:\\Users\\woong\\Zotero\\storage\\GQVUPCZ9\\Taylor et al. - 2017 - A deep learning approach for generalized speech an.pdf}
}

@article{tinwellFacialExpressionEmotion2011,
  title = {Facial Expression of Emotion and Perception of the {{Uncanny Valley}} in Virtual Characters},
  shorttitle = {{{UncannyVAlly}}},
  author = {Tinwell, Angela and Grimshaw, Mark and Nabi, Debbie Abdel and Williams, Andrew},
  year = {2011},
  month = mar,
  journal = {Computers in Human Behavior},
  series = {Web 2.0 in {{Travel}} and {{Tourism}}: {{Empowering}} and {{Changing}} the {{Role}} of {{Travelers}}},
  volume = {27},
  number = {2},
  pages = {741--749},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2010.10.018},
  abstract = {With technology allowing for increased realism in video games, realistic, human-like characters risk falling into the Uncanny Valley. The Uncanny Valley phenomenon implies that virtual characters approaching full human-likeness will evoke a negative reaction from the viewer, due to aspects of the character's appearance and behavior differing from the human norm. This study investigates if ``uncanniness'' is increased for a character with a perceived lack of facial expression in the upper parts of the face. More important, our study also investigates if the magnitude of this increased uncanniness varies depending on which emotion is being communicated. Individual parameters for each facial muscle in a 3D model were controlled for the six emotions: anger, disgust, fear, happiness, sadness and surprise in addition to a neutral expression. The results indicate that even fully and expertly animated characters are rated as more uncanny than humans and that, in virtual characters, a lack of facial expression in the upper parts of the face during speech exaggerates the uncanny by inhibiting effective communication of the perceived emotion, significantly so for fear, sadness, disgust, and surprise but not for anger and happiness. Based on our results, we consider the implications for virtual character design.},
  langid = {english},
  keywords = {Characters,Emotion,Facial expression,Realism,Uncanny Valley,Video games},
  file = {C\:\\Users\\woong\\Zotero\\storage\\IUMAZ72W\\Tinwell et al. - 2011 - Facial expression of emotion and perception of the.pdf;C\:\\Users\\woong\\Zotero\\storage\\HQRLL7W8\\S074756321000316X.html}
}

@article{zhouVisemenetAudiodrivenAnimatorcentric2018,
  title = {Visemenet: Audio-Driven Animator-Centric Speech Animation},
  shorttitle = {Visemenet},
  author = {Zhou, Yang and Xu, Zhan and Landreth, Chris and Kalogerakis, Evangelos and Maji, Subhransu and Singh, Karan},
  year = {2018},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {37},
  number = {4},
  pages = {161:1--161:10},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201292},
  abstract = {We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.},
  keywords = {facial animation,neural networks},
  file = {C\:\\Users\\woong\\Zotero\\storage\\G9U5FXT4\\Zhou et al. - 2018 - Visemenet audio-driven animator-centric speech an.pdf}
}




@misc{jangCategoricalReparameterizationGumbelSoftmax2017,
      title={Categorical Reparameterization with Gumbel-Softmax}, 
      author={Eric Jang and Shixiang Gu and Ben Poole},
      year={2017},
      eprint={1611.01144},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}