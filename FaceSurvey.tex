% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\graphicspath{{./figures/}}
\begin{document}

%%%%%%%%% TITLE
\title{Generating Expressive Facial Mesh Animation : A Survey}

\author{
%HJW\\
% CAU Univ.\\
{\tt\small}
}
\maketitle

%%%%%%%%% ABSTRACT
%\begin{abstract}
%With technology allowing for increasing realism in games and movies, facial animation is still a very challenging task. We conducted a survey in expressive face animations.
%\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

% Background
% Topic : Face anim, lip, 다시 각광... VR.... 러닝방법...
% Importance => 바로 industry에 적용됨
%  Why review...
% Goal
% 각 방법론(ling, dnn)의 장단점

% History 방법당 1~2문장
%  다른 서베이??
% 

Human tend to be very sensitive to facial motion psychologically. Slightest uncanniest in facial animation directly leads to hurt overall experience\cite{hansonUpendingUncannyValley}. Facial animation can be applied to multiple applications such as computer games, e-commerce, immersive VR telepresence, and movies. Yet achieving realistic facial animation is a challenging task. So, delivering natural expressive facial animation is a great interest in graphics field.

Animating high-quality expressive face is very labor-intensive job when done by animator. Another approach to animate face is to capture human face animation in 3D. Face capture is a well-understood field(cite here), yet such approach requires gigabytes of data from expensive capture system, and is hard to manipulate. Therefore, it is necessary to simplify such process. 

To simplify such process, one can automatically generate facial animation or can simplify animating produce.

In this survey, We introduce and compare three research that animate expressive facial animation :
\begin{itemize}
 \item JALI\cite{edwardsJALIAnimatorcentricViseme2016} and VisimeNet\cite{zhouVisemenetAudiodrivenAnimatorcentric2018}, a linguistic approach to lip-sync.
 \item MeshTalk\cite{richardMeshTalk3DFace2021}, a deep learning method.
 \item D3DExpression\cite{potamiasLearningGenerateCustomized2020}, LSTM method which replicate facial expression.
\end{itemize}


% Creating lip-sync animation is complex and challenging task.
% Mapping face expression to latent space is actively being researched due to recent advancement in deep learning field.



\section{Methods}

\subsection{JALI and Visemenet}

\subsubsection{JALI}

\begin{figure}
   \includegraphics[width=1.0\linewidth]{jaliStyles}
   
   \caption{Speaking styles of JALI viseme field}
   \label{fig:jaliStyles}
\end{figure}

(background)

(topic)

2. 
(linguitics based lip sync introduction)

()

(  )

(  )


%rule-based

%Introducing: a linguistic approach, an old approach
Lip-sync can be done by linguistic approach which is mapping text to phonemes, then phonemes to visemes\cite{ezzatMikeTalkTalkingFacial1998}. Viseme is a specific facial shape when making certain sound. A traditional facial rig can have many-to-one mapping from phonemes to visemes, or many-to-many using dynamic visemes. This approach can achieve realistic acoustic motion on the model.

%Introduce JALI
JALI\cite{edwardsJALIAnimatorcentricViseme2016} is a state-of-the-art viseme model that can generate lip-sync animation from English text and voice. JALI takes jaw and lip activation multipliers into consideration, since jaw and lip is the most significant acoustic motion in face. As shown on \cref{fig:jaliStyles}, different speaking styles shows different jaw lip activation level multipliers, which can be animated more intuitively.

%about linguistic model, suppressor rules etc
JALI use text-to-phoneme speech library built in to OSX. This converts English text into a phonemic representation. Next, text is forced aligned, which is a process that aligning phonemes to text. Audio must be annotated with the beginning, middle, and end of each phoneme. This is typically done by training Hidden Markov Model. Several tools exist for this task such as HTK and SPHINX.

JALI animates a facial rig by producing sparse animation keyframes for visemes, by some linguistic rules. When animating, lexical stress and co-articulation rule is considered. Co-articulation is a rule which visemes interact with each other. For example, duplicated visemes such as /p/ and /m/ in "pop man" are co-articulated into one MMM viseme, and tongue-only visemes have no influence on the lips, and lip takes shape of the surrounding visemes.

%Volume of each word have to be considered to determine which word is stressed. It is impossible to determine volume of plosive (e.g. p b d) or fricative (e.g. s z f). But vowel (a e i o u) is always voiced, so stress can be easily determined.

%Strength of JALI
Unlike other methods, JALI can produce sparse keyframed procedural animation. Generated animation can be edited to achieve more idiosyncratic animation. It is also easy to accompany the method with other animation, since animation only affects jaw and lip parameter of the rig.

%Limitation of JALI
Limitation of the work is that it lacks some auditory parameter such as rate of speech, and jitter and shimmer. Also, emotional speech styles are not considered. Emotional styles can affect subtle parameter, but is a critical part of expressiveness of the animation.

\subsubsection{Visemenet}

\begin{figure*}
   \centering
   \includegraphics[width=1.0\linewidth]{VisemenetArchitecture}
   \caption{Architecture of Visemenet model.}
   \label{fig:VisemenetArchitecture}
\end{figure*}


%Significance of Audio to animation.

%Introdude Visemenet
Visemenet is a deep-learning based lip-sync method based on JALI model. Unlike JALI, Visemenet takes auditory data only to generate JALI lip-sync animation. Visemenet tries to replicate linguistic model with multiple deep-learning LSTM layers.

%network overview
As shown on \cref{fig:VisemenetArchitecture}, Visemenet mainly have three stages. Phoneme group stage, landmark stage, and viseme stage. Phoneme and landmark stage is a pretrained model with relatively large amount of dataset, and Viseme stage a main model which predicts three types of JALI animation parameters.

%Pretrain network And datastet
Auditory data is converted to multiple features that has been used in previous studies. 65 feature vectors of 3 type per each frame is used, and 24 frames are used as input of all network, sum 1560 dimension.

Phoneme groups are 20 groups of phonemes which corresponds to certain visemes. Phoneme group stage estimates phoneme group from audio features.

38 artist-defined landmarks in real face with jaw, lip and nose is considered in landmark stage. Landmark stage estimates 2D-displacement of this landmark, resulting in 76 dimension output.

Pretrain networks are trained with about total 15 hour worth of publically available audiovisual facial dataset. Dataset used in training must have English transcript and corresponding voiced video. Most of the video dataset available used for facial recognition has transcript and high quality video recorded with camera at front of the face.

%Main network And dataset
Viseme stage is a main network that predicts JALI model variables. Input of the network is 1560 dimension audio features and 20 dimension predicted phoneme groups, and 76d landmark displacement groups. JALI control activation and visemes, co-articulation parameters are predicted in individual LSTM networks. LSTM layers have 256-dimension memory state, with 2 fully connected decoders.

Viseme stage is trained with 1 hour of rig motion curves. Experienced animator created dataset corresponding audio.

%Pros of Visemenet
Visemenet successfully mapped audio to speech motion curve. Pretrained network used hand-engineered audio features, replacing such features with learned feature will improve performance, which is happening at image processing.

%Limitation of Visemenet
This research mixes animator-centric technique and deep-learning method. But unlike JALI that shares low-dimension output, Visemenet generates frame-by-frame animation, which is harder to manipulate.

\subsection{MeshTalk}

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.8\linewidth]{meshtalk_overview.png}
   \end{center}
   \caption{The network diagram\cite{richardMeshTalk3DFace2021}.}
   \label{fig:long}
   \label{fig:networkdiag}
 \end{figure}

%Introduce Meshtalk
MeshTalk is a generic method for generating full facial mesh animation from speech and a expression mesh. Since speech does not encode all aspects of the facial expressions. Eye-blinks are a simple example of uncorrelated expressive information. Most existing audio-driven approaches static upper face animation. To overcome this issue, Meshtalk introduces a method that learn a categorical latent space for facial expressions.

%network overview
The network resembles Variational Autoencoder with multiple latent space as shown in Figure \ref{fig:networkdiag}.
Target mesh \(\hat{h}\) is estimated from template mesh \(h\) and latent space \(\mathsf{c}\) by computing function \(\mathcal{D}\).
\begin{equation}
  \hat{h}_{1:T} = \mathcal{D}(h, \mathsf{c}_{1:T, 1:H})
  \label{eq:1}
\end{equation}
Sequence of latent space \(\mathsf{c}_{1:T}\) is derived from audio sequence \(\mathsf{a}_{1:T}\) and expression signal mesh sequence \(\mathsf{x}_{1:T}\).
\(c\) and \(a\) are first mapped to \(T*H*C\) dimensional latent space, then passed through Gumbel-softmax\cite{jangCategoricalReparameterizationGumbelSoftmax2017} over every classification head.

Application of Meshtalks are re-targeting animated mesh to other static mesh, and re-synthesizeing sentences with a new language audio snippet.

%Dataset
In traning phase, the method uses In-house audio-visual dataset. At dataset, 1.4 million frames of 13 hours equivalent audio-visual dataset from 250 subjects, reading 50 phonetically balanced sentences.


%Pros of MeshTalk
A novel categorical latent space in combination with a cross-modality loss enables generation of highly realistic animation. Meshtalk demostrates high-accuracy lip mition and plausible motion of eye and eyebrows.

%Limitation of MeshTalk
The limitation of Meshtalk is that it cannot compenstate for hidden body parts. For example, hair covering eyebrow. And network size is modereltly large and cannot be run realtime at low-cost computational devices such as mobile phone or VR devices.

\subsection{D3DExpression}


%Introduction 
The goal of paper \emph{Learning to Generate Customized Dynamic 3D Facial Expressions}(D3DExpression) is to generate realistic 3D facial animation given a target expression and a static neutral face.

%network overview



\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.8\linewidth]{d3dphase.png}
   \end{center}
   \caption{Four phases of emotion, \cite{potamiasLearningGenerateCustomized2020}}
   \label{fig:d3dphase}
 \end{figure}

%dataset
Dataset is 4DFAB\cite{cheng4DFABLargeScale2018a}, which includes video clips with highly exaggerated six facial expressions, namely happy, sad, surprise, angry, disgust and fear, for 180 subjects. It is assumed that each expression can be characterised by four phases of its evolution as shown on \ref{fig:d3dphase}. Starting from Netural value of 0, onset that linerlly increment value, then apex value of 1, offset that decreses then back to netural face.

%model
The overall architecture of the model is structured by two major components. The first one contains a temporal encoder, using an LSTM layer that encodes the expected facial motion of the target expression. Input of the LSTM network is 6 dimension, which is a one-hot encoding of one of the six exprssion, with amplitude. The second component is a frame decoder, with four layers of mesh convolutions, where each one is followed by an upsampling layer. Each upsampling layer increases the number of vertices by five times, and every mesh convolution is followed by a ReLU activation. Finally, the output of the decode is added to the neutral face.

%Pros of D3DExpression
The model can predict exaggerated facial mesh, and interpolate between  diffrent expressions. Results show that the proposed method outperforms expression blendshapes. But the dataset is limited to extreme variation of the expression and limited to six emotions.


%Overview
%Limitation of D3DExpression

\subsection{Discussion}

We went through methods that generates rich facial animation. JALI takes phonemic approach to generate realistic animator centric lip-sync animation. Visemenet took this approach, an replaced phonemic component with deep learning layers, further automating the process. MeshTalk suggests novel loss function and categorical latent space to encourage the model to have an accurate upper and lower face reconstruction. D3DExpression proposed a method that can recreate facial expression animation.

\begin{table*}
   \centering
   \begin{tabular}{c|cc}
      \toprule
      Method & Input & Output \\
      \midrule
      JALI & Audio, transcript & JALI rigged keyframed lip-sync animation \\
      Visemenet & Audio & JALI rigged lip-sync animation \\
      MeshTalk & Audio, Expression mesh animation, template mesh & Mesh animation\\
      D3DExpression & Expression encoding, template mesh & Mesh animation with given expression\\
      \bottomrule
   \end{tabular}
   \caption{Comparison of input and output of each method}
   \label{table:io_cmp}
\end{table*}

Three of the paper targets lip-sync animation as a goal and shows high quality results (see \cref*{table:io_cmp}). Lip-sync is well understood field, but expressive animation of upper face like eyes and eyebrows is relatively unexplored, yet shows potential.

Meshtalk and D3DExpression generates mesh animation. Yet rendering mesh requires various texture and maps, like bump-map for wrinkles. Further research can be conducted to generate such textures.

\begin{table*}
   \centering
   \begin{tabular}{c|cc}
      \toprule
      Method & Types & Data(in time) \\
      \midrule
      JALI & No training data & - \\
      Visemenet & Publically available Audio, Transcript, Video & 15h pretrain 1h main \\
      MeshTalk & In-house Audio, Mesh animation & 13h \\
      D3DExpression & Expression labeled mesh animation & 1000+ clips\\
      \bottomrule
   \end{tabular}
   \caption{Comparison of traning dataset of each method}
   \label{table:data_cmp}
\end{table*}
%compare input and output with sheet

%compare training data with sheet

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{refs}
}

\end{document}
